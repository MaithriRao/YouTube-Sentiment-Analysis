{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc3130-7126-4678-a3c1-bcee374938df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340280ce-3254-44c1-8ed3-c3942972de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Himanshu-1703/reddit-sentiment-analysis/refs/heads/main/data/reddit.csv')\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8922f-c113-41b6-bcff-bbb94180109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9dd899-f280-4ac0-9488-6b89fea3fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773e5bd-f9a5-4194-978d-2a52a56d4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['clean_comment'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1a179-4ed0-46cf-8a35-739a079df79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['clean_comment'].isna()]['category'].value_counts()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78163e1-f90e-4016-9038-5827f5debc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e509d79-2741-470e-9197-df175014bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2a090-b3ba-4b3d-852f-6f2478416fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d8a26-9f69-4b38-8f51-6b626f2d79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a8a33-8b3c-4519-906e-b6ca391c0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['clean_comment'].str.strip() == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecc23f-3995-49a1-95b5-96b81903865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df[~(df['clean_comment'].str.strip() == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53164238-a2ab-40e4-bff0-570f84a5443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_comment'] = df['clean_comment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa63904-9d4e-49db-9108-c3ca8b2e9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e502487-8b45-40f0-b5e0-7bf61973d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['clean_comment'].apply(lambda x: x.startswith(' ') or x.endswith(' '))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953d57a-ed90-4d2b-b672-8ef339e9db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_comment'] = df['clean_comment'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42770300-ca0b-4dd4-b1ce-a55b5709f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['clean_comment'].apply(lambda x: x.startswith(' ') or x.endswith(' '))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ca3ba-7fea-46a8-9933-e1cc646b8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee6804-7442-42e5-88d1-1d03c2888a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify comments containing URLs\n",
    "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "comments_with_urls = df[df['clean_comment'].str.contains(url_pattern, regex=True)]\n",
    "\n",
    "# Display the comments containing URLs\n",
    "comments_with_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865dc217-c51e-4122-bc72-10bd5dc01b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify comments containing new line characters\n",
    "comments_with_newline = df[df['clean_comment'].str.contains('\\n')]\n",
    "\n",
    "# Display the comments containing new line characters\n",
    "comments_with_newline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87c586-b1c4-454b-8a66-d12af3e01094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new line characters from the 'clean_comment' column\n",
    "df['clean_comment'] = df['clean_comment'].str.replace('\\n', ' ', regex=True)\n",
    "\n",
    "# Verify the transformation by checking for any remaining new lines\n",
    "comments_with_newline_remaining = df[df['clean_comment'].str.contains('\\n')]\n",
    "comments_with_newline_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79234201-ccdc-4404-89f7-c283ef4927cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# distribution of classes\n",
    "\n",
    "sns.countplot(data=df,x=\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccacdb-9dff-41ac-95f4-55853d4b4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a new column 'num_stop_words' by counting the number of stopwords in each comment\n",
    "df['num_stop_words'] = df['clean_comment'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d12f0-f212-4fca-a408-a5fb29c2ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution of stop words in the 'clean_comment' column\n",
    "from collections import Counter\n",
    "\n",
    "# Extract all stop words from the comments using the previously defined 'common_stopwords'\n",
    "all_stop_words = [word for comment in df['clean_comment'] for word in comment.split() if word in stop_words]\n",
    "\n",
    "# Count the most common stop words\n",
    "most_common_stop_words = Counter(all_stop_words).most_common(25)\n",
    "\n",
    "# Convert the most common stop words to a DataFrame for plotting\n",
    "top_25_df = pd.DataFrame(most_common_stop_words, columns=['stop_word', 'count'])\n",
    "\n",
    "# Create the barplot for the top 25 most common stop words\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=top_25_df, x='count', y='stop_word', palette='viridis')\n",
    "plt.title('Top 25 Most Common Stop Words')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Stop Word')\n",
    "plt.show()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4419bc-81f1-447b-bd5d-5344bbaf8602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a7163-6108-4cf7-b91d-22b2043bb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove non-English characters from the 'clean_comment' column\n",
    "# Keeping only standard English letters, digits, and common punctuation\n",
    "import re\n",
    "\n",
    "df['clean_comment'] = df['clean_comment'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s!?.,]', '', str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce73d55-7a5e-4153-aecc-1ee50df2996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(df['clean_comment'])\n",
    "\n",
    "# Count the frequency of each character\n",
    "char_frequency = Counter(all_text)\n",
    "\n",
    "# Convert the character frequency into a DataFrame for better display\n",
    "char_frequency_df = pd.DataFrame(char_frequency.items(), columns=['character', 'frequency']).sort_values(by='frequency', ascending=False)\n",
    "\n",
    "char_frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f50863-6d7e-4fe4-beee-25d261b1b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Defining stop words but keeping essential ones for sentiment analysis\n",
    "stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
    "\n",
    "# Remove stop words from 'clean_comment' column, retaining essential ones\n",
    "df['clean_comment'] = df['clean_comment'].apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47367f-acd3-4500-b565-e1f77cf792ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to the 'clean_comment_no_stopwords' column\n",
    "df['clean_comment'] = df['clean_comment'].apply(\n",
    "    lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d887cf-44fd-4d00-9f9e-98615ec43b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_cloud(text):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_word_cloud(df['clean_comment'])\n",
    "\n",
    "# plot_word_cloud(df[df['category'] == 1]['clean_comment']) # for every category you can do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9178d-7549-4ac0-9ced-82006eda8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n_words(df, n=20):\n",
    "    \"\"\"Plot the top N most frequent words in the dataset.\"\"\"\n",
    "    # Flatten all words in the content column\n",
    "    words = ' '.join(df['clean_comment']).split()\n",
    "\n",
    "    # Get the top N most common words\n",
    "    counter = Counter(words)\n",
    "    most_common_words = counter.most_common(n)\n",
    "\n",
    "    # Split the words and their counts for plotting\n",
    "    words, counts = zip(*most_common_words)\n",
    "\n",
    "    # Plot the top N words\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(counts), y=list(words))\n",
    "    plt.title(f'Top {n} Most Frequent Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_top_n_words(df, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f266bd-07e5-4b0c-9e7e-e9552869d3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
